{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"k7.trainingLoopFromSkratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4zPx9eKfg3KPWKkkzfLr9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NgyBGxxw52xG","colab_type":"text"},"source":["# Writing a training loop from scratch\n"]},{"cell_type":"code","metadata":{"id":"HbBQJgNj5rIS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431307431,"user_tz":-330,"elapsed":2793,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7Hcc9p66DZF","colab_type":"text"},"source":["Forword Propagation"]},{"cell_type":"code","metadata":{"id":"O7lR6xX46AKr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431338290,"user_tz":-330,"elapsed":1606,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["inputs = keras.Input(shape=(784,), name=\"digits\")\n","x1 = layers.Dense(64, activation=\"relu\")(inputs)\n","x2 = layers.Dense(64, activation=\"relu\")(x1)\n","outputs = layers.Dense(10, name=\"predictions\")(x2)\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Toa0Q3cp6KaJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593431340708,"user_tz":-330,"elapsed":1906,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}},"outputId":"48d3fd5e-a875-43f1-b4b6-062ac30f1025"},"source":["# Instantiate an optimizer.\n","optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Prepare the training dataset.\n","batch_size = 64\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = np.reshape(x_train, (-1, 784))\n","x_test = np.reshape(x_train, (-1, 784))\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AjR-7bhh6U0L","colab_type":"text"},"source":["Backword Propagation"]},{"cell_type":"code","metadata":{"id":"rDhxG7kG6K7d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1593431405569,"user_tz":-330,"elapsed":11198,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}},"outputId":"d315e85a-24a6-425a-ec8f-52db341f6a0d"},"source":["epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","\n","        # Open a GradientTape to record the operations run\n","        # during the forward pass, which enables autodifferentiation.\n","        with tf.GradientTape() as tape:\n","\n","            # Run the forward pass of the layer.\n","            # The operations that the layer applies\n","            # to its inputs are going to be recorded\n","            # on the GradientTape.\n","            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n","\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y_batch_train, logits)\n","\n","        # Use the gradient tape to automatically retrieve\n","        # the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, model.trainable_weights)\n","\n","        # Run one step of gradient descent by updating\n","        # the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %s samples\" % ((step + 1) * 64))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 100.1588\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 1.3797\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.3948\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.5521\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.5361\n","Seen so far: 51264 samples\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.2842\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.2795\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.4321\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.2672\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.2964\n","Seen so far: 51264 samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5eQ9ZsH26g99","colab_type":"text"},"source":["#### Low-level handling of metrics"]},{"cell_type":"code","metadata":{"id":"V3uNHdOG6YfZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431474568,"user_tz":-330,"elapsed":1066,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["# Get model\n","inputs = keras.Input(shape=(784,), name=\"digits\")\n","x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n","x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n","outputs = layers.Dense(10, name=\"predictions\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Instantiate an optimizer to train the model.\n","optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Prepare the metrics.\n","train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","\n","# Prepare the training dataset.\n","batch_size = 64\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","# Prepare the validation dataset.\n","# Reserve 10,000 samples for validation.\n","x_val = x_train[-10000:]\n","y_val = y_train[-10000:]\n","x_train = x_train[:-10000]\n","y_train = y_train[:-10000]\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","val_dataset = val_dataset.batch(64)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbP73yhD6r0N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1593431499489,"user_tz":-330,"elapsed":13404,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}},"outputId":"7b0a9bc4-8855-4960-e2dd-e2b2d3c3c527"},"source":["import time\n","\n","epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        with tf.GradientTape() as tape:\n","            logits = model(x_batch_train, training=True)\n","            loss_value = loss_fn(y_batch_train, logits)\n","        grads = tape.gradient(loss_value, model.trainable_weights)\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","        # Update training metric.\n","        train_acc_metric.update_state(y_batch_train, logits)\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n","\n","    # Display metrics at the end of each epoch.\n","    train_acc = train_acc_metric.result()\n","    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    train_acc_metric.reset_states()\n","\n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_dataset:\n","        val_logits = model(x_batch_val, training=False)\n","        # Update val metrics\n","        val_acc_metric.update_state(y_batch_val, val_logits)\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 99.2577\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 1.4378\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.9464\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.8209\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.6004\n","Seen so far: 51264 samples\n","Training acc over epoch: 0.6966\n","Validation acc: 0.8166\n","Time taken: 6.27s\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.8488\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.9190\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.5716\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.3205\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.5478\n","Seen so far: 51264 samples\n","Training acc over epoch: 0.8285\n","Validation acc: 0.8728\n","Time taken: 6.20s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vlbkXF1i6_X3","colab_type":"text"},"source":["#### Speeding-up your training step with tf.function\n","\n"]},{"cell_type":"code","metadata":{"id":"J6A_7YvC6u4-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431609104,"user_tz":-330,"elapsed":1087,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["@tf.function\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    train_acc_metric.update_state(y, logits)\n","    return loss_value\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfdQKqRB7MqA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431622293,"user_tz":-330,"elapsed":1394,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["@tf.function\n","def test_step(x, y):\n","    val_logits = model(x, training=False)\n","    val_acc_metric.update_state(y, val_logits)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRAwHw9d7PlH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1593431643651,"user_tz":-330,"elapsed":4693,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}},"outputId":"f68c5c44-02d4-41b3-82e2-f9b46e9bce58"},"source":["import time\n","\n","epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        loss_value = train_step(x_batch_train, y_batch_train)\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n","\n","    # Display metrics at the end of each epoch.\n","    train_acc = train_acc_metric.result()\n","    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    train_acc_metric.reset_states()\n","\n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_dataset:\n","        test_step(x_batch_val, y_batch_val)\n","\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 0.7452\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.6140\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.4371\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.4541\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.7189\n","Seen so far: 51264 samples\n","Training acc over epoch: 0.8658\n","Validation acc: 0.8955\n","Time taken: 2.19s\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.6170\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.6102\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.1978\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.5734\n","Seen so far: 38464 samples\n","Training loss (for one batch) at step 800: 0.2718\n","Seen so far: 51264 samples\n","Training acc over epoch: 0.8820\n","Validation acc: 0.9047\n","Time taken: 1.73s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VAjh3q6Q7mS0","colab_type":"text"},"source":["Low-level handling of losses tracked by the model"]},{"cell_type":"code","metadata":{"id":"rQzrknes7UNY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431726735,"user_tz":-330,"elapsed":1442,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["class ActivityRegularizationLayer(layers.Layer):\n","    def call(self, inputs):\n","        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n","        return inputs"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3KFOCkH7pSa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431735745,"user_tz":-330,"elapsed":1657,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["inputs = keras.Input(shape=(784,), name=\"digits\")\n","x = layers.Dense(64, activation=\"relu\")(inputs)\n","# Insert activity regularization as a layer\n","x = ActivityRegularizationLayer()(x)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","outputs = layers.Dense(10, name=\"predictions\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"efqkO6jv7rb6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431744346,"user_tz":-330,"elapsed":1529,"user":{"displayName":"Tejas Sathe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnRon8zvA3-My2eEFYY3HUXCcGgAxxamBsPa9w=s64","userId":"15644987320264799783"}}},"source":["@tf.function\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","        # Add any extra losses created during the forward pass.\n","        loss_value += sum(model.losses)\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    train_acc_metric.update_state(y, logits)\n","    return loss_value\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPNhqbEB7tgu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}